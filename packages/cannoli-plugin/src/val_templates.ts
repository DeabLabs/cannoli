export const SYMBOLS = {
    canvasJSON: "@{1}",
    defaultGroqModel: "@{2}",
    defaultOpenaiModel: "@{3}",
    defaultGeminiModel: "@{4}",
    defaultAnthropicModel: "@{5}",
    defaultGroqTemperature: "@{6}",
    defaultOpenaiTemperature: "@{7}",
    defaultGeminiTemperature: "@{8}",
    defaultAnthropicTemperature: "@{9}",
    defaultOpenaiBaseURL: "@{10}",
    defaultGeminiBaseURL: "@{11}",
    defaultProvider: "@{12}",
    defaultModel: "@{13}",
} as const

export const cannoliValTemplate = `import { runCannoli, LLMProvider, ResponseTextFetcher, GetDefaultConfigByProvider } from "npm:@deablabs/cannoli-core@latest";

export default async function (req: Request): Promise<Response> {
  let args = {};
  if (req.method === "POST") {
    args = await req.json();
  } else if (req.method === "GET") {
    const sp = new URLSearchParams(req.url.split("?")[1]);
    args = Object.fromEntries(sp.entries());
  }
  
  // You only need to create and pass this function into the LLMProvider constructor if you need to use the default models,
  // temperatures, or base URLs of non-default providers.
  const getDefaultConfigByProvider = (provider) => {
   if(provider === "groq") {
     return {
        model: "${SYMBOLS.defaultGroqModel}",
        apiKey: Deno.env.get("GROQ_API_KEY"),
        temperature: "${SYMBOLS.defaultGroqTemperature}"
     }
   } else if (provider === "anthropic") {
      return {
        model: "${SYMBOLS.defaultAnthropicModel}",
        apiKey: Deno.env.get("ANTHROPIC_API_KEY"),
        temperature: "${SYMBOLS.defaultAnthropicTemperature}"
      }
   } else if (provider === "gemini") {
      return {
        model: "${SYMBOLS.defaultGeminiModel}",
        apiKey: Deno.env.get("GEMINI_API_KEY"),
        temperature: "${SYMBOLS.defaultGeminiTemperature}"
      }
   } else if (provider === "openai") {
      return {
        model: "${SYMBOLS.defaultOpenaiModel}",
        apiKey: Deno.env.get("OPENAI_API_KEY"),
        temperature: "${SYMBOLS.defaultOpenaiTemperature}"
      }
   }
     return {}
  }

const llm = new LLMProvider(
    {
      provider: "${SYMBOLS.defaultProvider}",
      getDefaultConfigByProvider
    }
);

const cannoliJson = ${SYMBOLS.canvasJSON}

const [liveStoppagePromise, stopLiveCannoli] = runCannoli({
    llm: llm,
    cannoliJSON: cannoliJson,
    args: args,
});

const stoppage = await liveStoppagePromise;
return Response.json(stoppage.results);
}
`;

export function cannoliValReadmeTemplate(name: string, url: string, args: Record<string, string>) {
    const argsList = Object.keys(args).map(arg => `- \`${arg}\``).join('\n');
    const exampleArgs = Object.keys(args).map(arg => `"${arg}": "value"`).join(',\n  ');
    const urlWithArgs = url + "?" + Object.keys(args).map(arg => `${arg}=value`).join('&');

    return `# ${name}

This is a Val generated by the Cannoli plugin for Obsidian. It runs a Cannoli where the floating nodes are populated by the parameters in the request, and the response includes the contents of the floating nodes after the Cannoli has completed.

## Usage

This Val can be used as an HTTP endpoint with either GET or POST methods.

### GET Request

For a GET request, include the parameters in the URL query string. Each parameter will replace the corresponding floating node content.

**Example:**
\`\`\`
GET [${urlWithArgs}](${urlWithArgs})
\`\`\`

### POST Request

For a POST request, send a JSON object in the request body. The keys in the JSON object will identify which floating node to inject, and the values will be the content to inject.

**Example:**
\`\`\`
curl -X POST ${url} \\
  -H "Content-Type: application/json" \\
  -d '{
  ${exampleArgs}
}'
\`\`\`

### Parameters

The following parameters can be used with this Val:
${argsList}

### Response

The response will be a JSON object where the keys are the floating node names and the values are the contents of the floating nodes after the Cannoli has been run. These keys will match the parameters provided in the request.

**Example Response:**
\`\`\`json
{
  ${exampleArgs}
}
\`\`\`

## Security

Since this Val has access to your AI provider API keys from the environment, please do not set the privacy to "Public".

If you'd like others to be able to use this Val, create a new environment variable to act as a key, modify the Val to accept a key as an argument, and only allow the Cannoli to run after that key is checked. Then you can set the Val to "public" or "unlisted".
`;
}

